{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Karaoke Scoring System**\n",
    "\n",
    "### **Overview:**\n",
    "The Karaoke Scoring System is meticulously designed to evaluate a user's singing performance against an original track. Utilizing advanced audio processing techniques and alignment strategies, it delivers precise and insightful scoring, ensuring users gain deep insights into their performance.\n",
    "\n",
    "### **KaraokeData:**\n",
    "At the core of our system is the `KaraokeData` class, serving as the single access point for essential data for a particular song: the original singer's audio, the instrumental track, and synchronized lyrics. Beyond just storage, this class adeptly parses lyrics into a structured format, ensuring time-specific lyric extraction, which is paramount for aligning user feedback with distinct moments in the song.\n",
    "\n",
    "#### **Utilization Within KaraokeData:**\n",
    "- The **original singer's audio** sets the standard for user performance comparisons.\n",
    "- The **instrumental track** is instrumental in audio preprocessing, aiding in identifying and attenuating background noises.\n",
    "- **Synchronized lyrics** enhance the user experience, providing context to the feedback and ensuring precision in alignment.\n",
    "\n",
    "### **AudioPreprocessor:**\n",
    "The `AudioPreprocessor` class refines the user's audio through:\n",
    "1. **Normalization**: Adjusting the audio to have zero mean and unit variance.\n",
    "2. **Silence Trimming**: Removing any leading and trailing silences from the user's audio.\n",
    "3. **Spectral Gate**: Filtering out frequencies below a threshold, significantly reducing low-level noise.\n",
    "4. **Adaptive Noise Reduction**: Harnessing the instrumental track to pinpoint and eliminate background noise from the user's audio.\n",
    "5. **Voice Activity Detection (VAD)**: Spotting segments where the user is actively singing, ensuring the vocal's prominence over potential background disturbances.\n",
    "\n",
    "### **Scoring Mechanisms:**\n",
    "Our system leverages diverse metrics to deliver a well-rounded evaluation:\n",
    "1. **Linguistic Accuracy Score**: Employs Google's Speech Transcription service to transcribe the user's audio to text. This transcribed text is then matched with the original lyrics, determining pronunciation and word accuracy.\n",
    "2. **Amplitude Matching Score**: Utilizes Dynamic Time Warping (DTW) to compare amplitude profiles between the user's audio and the original.\n",
    "3. **Pitch Matching Score**: Investigates the fundamental frequency contours of both the user's and original audio, ensuring tonal alignment.\n",
    "4. **Rhythm Score**: Contrasts onset patterns between the user's performance and the original, assessing synchronization and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_vis import AudioVis\n",
    "from karaoke_data import KaraokeData\n",
    "from audio_scorer import AudioScorer\n",
    "from audio_preprocessor import AudioPreprocessor\n",
    "from google_speech_transcription import GoogleSpeechTranscription\n",
    "\n",
    "av = AudioVis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_dir = \"data\"\n",
    "lyrics_dir = os.path.join(base_dir, \"SongsLyrics\", \"Lyrics\")\n",
    "track_dir = os.path.join(base_dir, \"SongsLyrics\", \"Track\")\n",
    "voice_dir = os.path.join(base_dir, \"SongsLyrics\", \"Voice\")\n",
    "\n",
    "# Generate the dictionary\n",
    "data_dict = {}\n",
    "\n",
    "# Fill attempted songs\n",
    "for song_file in os.listdir(base_dir):\n",
    "    if song_file.endswith(\".wav\"):\n",
    "        song_id = os.path.splitext(song_file)[0]\n",
    "        data_dict[song_id] = {\"Attempted\": os.path.join(base_dir, song_file)}\n",
    "\n",
    "# Fill lyrics\n",
    "for lyrics_file in os.listdir(lyrics_dir):\n",
    "    song_id = lyrics_file.split('_')[0]\n",
    "    if song_id in data_dict:\n",
    "        data_dict[song_id][\"Lyrics\"] = os.path.join(lyrics_dir, lyrics_file)\n",
    "\n",
    "# Fill tracks\n",
    "for track_file in os.listdir(track_dir):\n",
    "    song_id = os.path.splitext(track_file)[0]\n",
    "    if song_id in data_dict:\n",
    "        data_dict[song_id][\"Track\"] = os.path.join(track_dir, track_file)\n",
    "\n",
    "# Fill voices\n",
    "for voice_file in os.listdir(voice_dir):\n",
    "    if \"voice_1\" in voice_file:\n",
    "        song_id = voice_file.split('_')[0]\n",
    "        if song_id in data_dict:\n",
    "            data_dict[song_id][\"Original\"] = os.path.join(voice_dir, voice_file)\n",
    "    elif \"voice_2\" in voice_file:\n",
    "        song_id = voice_file.split('_')[0]\n",
    "        if song_id in data_dict:\n",
    "            data_dict[song_id][\"Original Second\"] = os.path.join(voice_dir, voice_file)\n",
    "    else:\n",
    "        song_id = os.path.splitext(voice_file)[0]\n",
    "        if song_id in data_dict:\n",
    "            data_dict[song_id][\"Original\"] = os.path.join(voice_dir, voice_file)\n",
    "\n",
    "# Print a sample\n",
    "print(data_dict.get(\"42029\", {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "all_files_count = 0\n",
    "only_attempt_count = 0\n",
    "only_original_count = 0\n",
    "\n",
    "usable_ids = []\n",
    "\n",
    "# Iterate through the dictionary to count\n",
    "for song_id, song_data in data_dict.items():\n",
    "    if \"Attempted\" in song_data and \"Lyrics\" in song_data and \"Track\" in song_data and (\"Original\" in song_data or \"Original Second\" in song_data):\n",
    "        usable_ids.append(song_id)\n",
    "        all_files_count += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of IDs with all files: {all_files_count}\")\n",
    "print(usable_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_data(song_id):\n",
    "    song_data = data_dict.get(song_id, {})\n",
    "    if \"Attempted\" in song_data and \"Lyrics\" in song_data and \"Track\" in song_data and (\"Original\" in song_data or \"Original Second\" in song_data):\n",
    "        original_audio, sr = librosa.load(song_data['Original'], sr=None, mono=True)\n",
    "        attempted_audio, sr = librosa.load(song_data['Attempted'], sr=None, mono=True)\n",
    "        track_audio, sr = librosa.load(song_data['Track'], sr=None, mono=True)\n",
    "        return original_audio, attempted_audio, track_audio, song_data['Lyrics'], sr\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get the song data\n",
    "song_data = get_song_data(\"44957\")\n",
    "print(song_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_audio, attempted_audio, track_audio, raw_lyrics_data, sr = get_song_data(\"27256\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = AudioVis()\n",
    "\n",
    "av.wav_plot(original_audio, sr, title=\"Original Audio\")\n",
    "av.play_audio(original_audio, sr)\n",
    "\n",
    "av.wav_plot(attempted_audio, sr, title=\"Attempted Audio\")\n",
    "av.play_audio(attempted_audio, sr)\n",
    "\n",
    "av.wav_plot(track_audio, sr, title=\"Track Audio\")\n",
    "av.play_audio(track_audio, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simulate receiving audio in chunks, I have created split_into_chunks\n",
    "def split_into_chunks(audio, num_chunks=5):\n",
    "    \"\"\"Splits the audio data into a specified number of chunks.\"\"\"\n",
    "    chunk_size = len(audio) // num_chunks\n",
    "    chunks = [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]\n",
    "    return chunks[:num_chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(attempted_audio, 10)\n",
    "\n",
    "chunk = chunks[0]\n",
    "av.wav_plot(chunk, sr, title=\"Original Audio\")\n",
    "av.play_audio(chunk, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KaraokeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KaraokeData\n",
    "karaoke_data = KaraokeData(original_audio=original_audio, track_audio=track_audio, raw_lyrics_data=raw_lyrics_data, sampling_rate=sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Lyrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_lyrics = karaoke_data.lyrics_data\n",
    "print(parsed_lyrics[:5])  # Displaying the first 5 parsed lyric entries for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "karaoke_data.align_audio(chunk, method=\"start\")\n",
    "print(f\"Position after start alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align Using Lyrics Data: This method uses the first entry in the parsed lyrics data to align the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "karaoke_data.align_audio(chunk, method=\"lyrics_data\")\n",
    "print(f\"Position after lyrics data alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align Using Onset Detection:\n",
    "This method aligns the audio by detecting onsets in both the original audio and the provided audio chunk. It then attempts to align the first onset of the chunk with the corresponding onset in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "karaoke_data.align_audio(chunk, method=\"onset_detection\")\n",
    "print(f\"Position after onset detection alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align Using Cross-Correlation:\n",
    "This method computes the cross-correlation between the original audio and the provided audio chunk to find the best alignmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karaoke_data.reset_alignment()  # Resetting any prior alignments\n",
    "karaoke_data.align_audio(chunk, method=\"cross_correlation\")\n",
    "print(f\"Position after cross-correlation alignment: {karaoke_data.current_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Segment Retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length = len(chunk)  # Using the length of the first audio chunk\n",
    "retrieved_original_segment, retrieved_track_segment = karaoke_data.get_next_segment(segment_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av.wav_plot(chunk, sr, title=\"Chunk Audio\")\n",
    "av.play_audio(chunk, sr)\n",
    "\n",
    "av.wav_plot(retrieved_original_segment, sr, title=\"Original Audio\")\n",
    "av.play_audio(retrieved_original_segment, sr)\n",
    "\n",
    "av.wav_plot(retrieved_track_segment, sr, title=\"Track Audio\")\n",
    "av.play_audio(retrieved_track_segment, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_lyrics = karaoke_data.get_lyrics()\n",
    "print(segment_lyrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Audio Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AudioPreprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_effect(before, after, sr, effect_name, visualization_functions):\n",
    "    \"\"\"\n",
    "    Demonstrates the effect of a preprocessing function by playing and visualizing:\n",
    "    - The original audio\n",
    "    - The processed audio\n",
    "    - (Optional) The removed audio (difference between the original and processed audio)\n",
    "    - Visualizations specified in visualization_functions for each of the audios\n",
    "    \"\"\"\n",
    "    # Play original audio\n",
    "    print(f\"Original Audio ({effect_name}):\")\n",
    "    av.play_audio(before, sr)\n",
    "\n",
    "    # Play processed audio\n",
    "    print(f\"\\nTransformed Audio ({effect_name}):\")\n",
    "    av.play_audio(after, sr)\n",
    "\n",
    "    same_length = len(before) == len(after)\n",
    "\n",
    "    # If the lengths are the same, play the difference audio\n",
    "    if same_length:\n",
    "        difference = before - after\n",
    "        print(f\"\\nRemoved Audio ({effect_name}):\")\n",
    "        av.play_audio(difference, sr)\n",
    "\n",
    "    # Display visualizations\n",
    "    for viz_func in visualization_functions:\n",
    "        print(f\"\\nOriginal Audio - {effect_name}:\")\n",
    "        viz_func(before, sr)\n",
    "\n",
    "        print(f\"\\nTransformed Audio - {effect_name}:\")\n",
    "        viz_func(after, sr)\n",
    "\n",
    "        # If the lengths are the same, visualize the difference audio\n",
    "        if same_length:\n",
    "            print(f\"\\nDifference - {effect_name}:\")\n",
    "            viz_func(difference, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim Audio\n",
    "\n",
    "Description: Trimming silences involves removing any leading or trailing silent parts from an audio signal. This can be useful to eliminate unnecessary silent portions which don't contribute to the actual content.\n",
    "\n",
    "Implementation: The trim_audio function uses the librosa.effects.trim function to achieve this. The top_db parameter defines a threshold in decibels below which the audio is considered silent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vf = [av.wav_plot, av.plot_spectrogram, av.plot_mfcc]\n",
    "vf = [av.wav_plot]\n",
    "trimmed_chunk = ap.trim_audio(chunk)\n",
    "# demonstrate_effect(chunk, trimmed_chunk, sr, \"Trimming\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Audio\n",
    "\n",
    "Description: Normalization adjusts the audio amplitude so that its average amplitude is zero, and its standard deviation is one. This ensures that the audio's loudness is relatively consistent, which can be beneficial for further processing or analysis.\n",
    "\n",
    "Implementation: The _normalize_segment function subtracts the mean from the audio segment and then divides by the standard deviation. The normalize_audio function can normalize the entire audio or perform segment-wise normalization if a segment_length is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vf = [av.wav_plot, av.plot_spectrogram, av.plot_mfcc]\n",
    "normalized_chunk = ap.normalize_audio(chunk)\n",
    "demonstrate_effect(chunk, normalized_chunk, sr, \"Normalization\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Gate\n",
    "\n",
    "Description: This involves suppressing frequency components of the signal below a certain threshold. It helps in reducing noise or undesired frequencies.\n",
    "\n",
    "Implementation: In the spectral_gate function, an STFT (Short-Time Fourier Transform) is performed, and any frequencies below the threshold are set to zero. The processed signal is then reconstructed using the inverse STFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_gated_chunk = ap.spectral_gate(chunk, threshold=0.1)\n",
    "demonstrate_effect(chunk, spectral_gated_chunk, sr, \"Spectral Gating\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Noise Reduction\n",
    "\n",
    "Description: Adaptive noise reduction aims to reduce background noise from the user's audio using a reference (typically the instrumental track). By comparing the reference track with the user's audio, it identifies and subtracts common background elements, reducing interference or bleed from the instrumental.\n",
    "\n",
    "Implementation: In the given code, the method named spectral_masking is used for this purpose. It calculates a mask based on the ratio of magnitudes of the user audio to the combined magnitudes of the user and reference audios. This mask, when applied to the user's audio STFT, emphasizes the parts where the user's audio is dominant (like vocals) and suppresses the parts that are common with the reference (like instrumental bleed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptively_reduced_chunk = ap.adaptive_noise_reduction(chunk, retrieved_track_segment, sr)\n",
    "demonstrate_effect(chunk, adaptively_reduced_chunk, sr, \"Adaptive Noise Reduction\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice Activity Detection\n",
    "\n",
    "Description: VAD is employed to detect when a person is speaking/singing in an audio clip. This is valuable when you want to separate or focus on vocal content and exclude long silences or background noise.\n",
    "\n",
    "Implementation: The voice_activity_detection function uses the librosa.effects.split function, which identifies segments of the signal that are above a certain loudness threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_chunk = ap.voice_activity_detection(chunk, sr, top_db=5)  # Adjust the top_db value as needed\n",
    "demonstrate_effect(chunk, vad_chunk, sr, \"Voice Activity Detection\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Source separation is the process of separating the main audio source from the rest of the audio. The method used here employs Non-negative Matrix Factorization (NMF) on the Mel spectrogram of the audio chunk. NMF factorizes the spectrogram into two matrices: the components matrix and the activations matrix. Each row of the components matrix can be thought of as a \"template\" spectrum, and the corresponding row of the activations matrix tells when that template is active.\n",
    "\n",
    "Implementation: In the method source_separation, the code computes the Mel spectrogram of the input audio chunk, then performs NMF to get the components and activations. The main audio source is identified as the component with the highest sum of activations, and it is then synthesized back into the time domain to produce the separated main audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_separation(audio_chunk: np.array, sr: int = 22050) -> np.array:\n",
    "    \"\"\"Separates the harmonic component using Harmonic/Percussive source separation.\"\"\"\n",
    "    # Separate harmonic and percussive components\n",
    "    harmonic, _ = librosa.effects.hpss(audio_chunk)\n",
    "    return harmonic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_separated_chunk = source_separation(chunk, sr)\n",
    "demonstrate_effect(chunk, source_separated_chunk, sr, \"Source Separation\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Masking\n",
    "\n",
    "Description: Spectral masking emphasizes certain frequency components based on a reference signal. This can help in reducing interference or background sounds.\n",
    "\n",
    "Implementation: The spectral_masking function calculates a mask based on the ratio of magnitudes of the user audio to the sum of magnitudes of the user and reference audios. This mask is then applied to the user's audio STFT, and the processed audio is reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_chunk = ap.spectral_masking(chunk, retrieved_track_segment)\n",
    "demonstrate_effect(chunk, masked_chunk, sr, \"Spectral Masking\", vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_pipeline(audio_chunk, pipeline, sr, **kwargs):\n",
    "    \"\"\"Demonstrates the effect of a preprocessing pipeline.\"\"\"\n",
    "    processed_audio = AudioPreprocessor.preprocess_audio(audio_chunk, pipeline, **kwargs)\n",
    "    pipeline_name = \" -> \".join(pipeline)\n",
    "    vf = [av.wav_plot]\n",
    "    demonstrate_effect(audio_chunk, processed_audio, sr, pipeline_name, vf)\n",
    "\n",
    "# Define the pipelines\n",
    "pipeline_1 = [\"normalize\"]\n",
    "pipeline_2 = [\"adaptive_noise_reduction\", \"normalize\"]\n",
    "pipeline_3 = [\"adaptive_noise_reduction\", \"source_separation\", \"normalize\"]\n",
    "\n",
    "# Additional arguments for the pipelines\n",
    "pipeline_args = {\n",
    "    \"adaptive_noise_reduction\": {\"reference_audio\": retrieved_track_segment}\n",
    "}\n",
    "\n",
    "# Apply and demonstrate each pipeline\n",
    "demonstrate_pipeline(chunk, pipeline_1, sr)\n",
    "demonstrate_pipeline(chunk, pipeline_2, sr, **pipeline_args)\n",
    "demonstrate_pipeline(chunk, pipeline_3, sr, **pipeline_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioScorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linguistic Accuracy**: The transcription is used to determine how closely the sung content matches the actual lyrics. This is a `qualitative measure`.\n",
    "\n",
    "**Amplitude, Pitch, and Rhythm Matching**: These are `quantitative measures`. They compare the user's sung audio features with the reference (original) audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = GoogleSpeechTranscription()\n",
    "\n",
    "#fastdtw is suppose to be much faster but has bug\n",
    "audio_scorer = AudioScorer(transcriber, 'dtaidistance_fast')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linguistic Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(karaoke_data.get_lyrics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.transcribe(chunk, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is because the audio is long, for short audio this will work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_score = audio_scorer.linguistic_accuracy_score(chunk, sr, segment_lyrics)\n",
    "print(f\"Linguistic Accuracy Score: {linguistic_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rhythm Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Rhythm score quantifies how closely the rhythm of a user's audio matches a reference audio. It can be computed using onset strength, which is a measure of the abruptness of sound changes.\n",
    "\n",
    "**Implementation**: It compute onset strength for both user audio and reference audio using the `librosa.onset.onset_strength` function. It then computes the Dynamic Time Warping (DTW) similarity between these onset strength sequences to generate a rhythm score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhythm_score = audio_scorer.rhythm_score(np.array(chunk), retrieved_original_segment)\n",
    "print(\"Rhythm Score:\", rhythm_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pitch Matching Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Pitch matching score assesses how closely the pitch contour of a user's audio aligns with that of a reference audio. Pitch contour is the variation of pitch over time.\n",
    "\n",
    "**Implementation**: Uses the `librosa.pyin` function to extract pitch sequences from the user audio and reference audio. It then computes the DTW similarity between these pitch sequences to yield the pitch matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_score = audio_scorer.pitch_matching_score(chunk, retrieved_original_segment)\n",
    "print(\"Pitch Matching Score:\", pitch_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude Matching Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: Amplitude matching score evaluates how well the amplitude envelope of a user's audio matches that of a reference audio.\n",
    "\n",
    "**Implementation**: Flattens the multi-dimensional audio arrays to 1D using `numpy.flatten`, then computes the DTW similarity between these 1D amplitude sequences to derive the amplitude matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_scorer = AudioScorer(transcriber, 'dtaidistance_fast')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_score = audio_scorer.amplitude_matching_score(chunk, retrieved_original_segment, sr)\n",
    "print(\"Amplitude Matching Score:\", amplitude_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = GoogleSpeechTranscription()\n",
    "\n",
    "class AudioProcessingPipeline:\n",
    "    def __init__(self, karaoke_data, attempted_audio, sr):\n",
    "        self.sr = sr\n",
    "        self.attempted_audio = attempted_audio\n",
    "        self.karaoke_data = karaoke_data\n",
    "        self.ap = AudioPreprocessor()\n",
    "        self.audio_scorer = AudioScorer(transcriber, 'dtaidistance_fast')\n",
    "\n",
    "    def process_and_score(self, original_pipeline, chunk_audio_pipeline):\n",
    "        total_scores = {\n",
    "            \"linguistic_score\": 0,\n",
    "            \"amplitude_score\": 0,\n",
    "            \"pitch_score\": 0,\n",
    "            \"rhythm_score\": 0,\n",
    "            }\n",
    "        num_chunks = 0\n",
    "        for chunk in split_into_chunks(self.attempted_audio, 20):\n",
    "            if num_chunks == 0:\n",
    "                self.karaoke_data.reset_alignment()\n",
    "                self.karaoke_data.align_audio(chunk, method=\"start\")\n",
    "            original_segment, track_segment = self.karaoke_data.get_next_segment(len(chunk))\n",
    "            original_processed = self.ap.preprocess_audio( original_segment, original_pipeline,)\n",
    "            chunk_processed = self.ap.preprocess_audio(chunk, chunk_audio_pipeline, reference_audio=track_segment)\n",
    "\n",
    "            scores = self.audio_scorer.process_audio_chunk(\n",
    "                chunk_processed, original_processed, self.karaoke_data.get_lyrics(), self.sr\n",
    "            )\n",
    "            num_chunks += 1\n",
    "            for score_name, score_value in scores.items():\n",
    "                total_scores[score_name] += score_value\n",
    "\n",
    "            print(scores)\n",
    "\n",
    "        # For now just computing average score\n",
    "        average_scores = {score_name: score_value / num_chunks for score_name, score_value in total_scores.items()}\n",
    "        return average_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_songs(usable_ids):\n",
    "    scores = {}\n",
    "    for song_id in usable_ids:\n",
    "        original_audio, attempted_audio, track_audio, raw_lyrics_data, sr = get_song_data(song_id)\n",
    "        karaoke_data = KaraokeData(\n",
    "            original_audio=original_audio,\n",
    "            track_audio=track_audio,\n",
    "            raw_lyrics_data=raw_lyrics_data,\n",
    "            sampling_rate=sr\n",
    "        )\n",
    "        audio_pipeline = AudioProcessingPipeline(karaoke_data, attempted_audio, sr)\n",
    "        average_score = audio_pipeline.process_and_score(\n",
    "            original_pipeline=[],\n",
    "            chunk_audio_pipeline=[]\n",
    "            # If any preproceesing is done google is returning empty transcription\n",
    "            # original_pipeline=[\"spectral_gate\", \"normalize\"],\n",
    "            # chunk_audio_pipeline=[\"adaptive_noise_reduction\", \"spectral_gate\", \"normalize\"]\n",
    "        )\n",
    "        scores[song_id] = average_score\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = process_all_songs(['27256', '58659'])\n",
    "for song_id, score in all_scores.items():\n",
    "    print(f\"Song ID: {song_id}, Average Score: {score}\")\n",
    "\n",
    "    # Playing the song\n",
    "    original_audio, _, _, _, sr = get_song_data(song_id)\n",
    "    AudioVis().play_audio(original_audio, sr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
